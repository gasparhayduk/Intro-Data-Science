help()
seq(from = 100, to = 200, by = 5)
c(1:100)
n <- c(2, 3, 5)
b <- c(TRUE, FALSE, TRUE, FALSE, FALSE)
lista1 <- list(n, b, 3)
lista2 <- list(numVar = n, logVar = b, scalar = 3)
lista2[[2]]
lista2$logVar
lista2$logVar[3]
c(1,3)
c(1, 3, 5) == 3
mtcars
?mtcars
mtcars$mpg > 20
mtcars[mtcars$mpg > 20,]
mtcars$mpg > 20 & mtcars$mpg <= 25
mtcars[mtcars$mpg > 20 & mtcars$mpg <= 25,]
c(2,6)
c(1, 2, 5, 10) %in% c(5, 8, 10)
mtcars$carb %in% c(2,6)
?grepl
mtcars[grepl("Merc", rownames(mtcars))
mtcars[grepl("Merc", rownames(mtcars)),]
mtcars[grepl("Merc", rownames(mtcars)),]
?mtcars
?setwd
bicis <- read.table("recorrido-bicis-2015.csv", header = TRUE,
sep = ";", stringsAsFactors = TRUE)
setwd("C:/Desktop/intro_datascience")
getwd()
setwd("C:/Users/gasparhayduk/Desktop/intro_datascience")
getwd()
strptime("22:05:32", "%H:%M:%OS")
Sys.time("2022-03-10 22:05:32 -03")
class(strptime("22:05:32", "%H:%M:%OS"))
?POSIXlt
as.numeric(strptime("22:05:32", "%H:%M:%OS"))
setwd("C:/Users/gasparhayduk/Home/Desktop/intro_datascience")
setwd("C:Home/Desktop/intro_datascience")
setwd("C:Users/gasparhayduk/Home/Desktop/intro_datascience")
rm(list=ls())
rm(list=ls())
library(LARF)
library(mgcv)
library(AER)
data("c401k", package = "LARF")
attach(c401k)
rm(list=ls())
data("c401k", package = "LARF")
attach(c401k)
View(c401k)
agee<-age-25
agee2<-agee^2
?larf
mod1 <- larf(nettfa ~ inc + agee + agee2 + marr + fsize, treatment = p401k  #el tratamiento, tomar el programa,
instrument = e401k, data = c401k)
mod1 <- larf(nettfa ~ inc + agee + agee2 + marr + fsize, treatment = p401k,
instrument = e401k, data = c401k)
summary(mod1)
firstStep <- gam(e401k ~ s(inc) + s(age) + s(agesq) + marr + s(fsize),
data=c401k, family=binomial(link = "probit"))
zProb <- firstStep$fitted
summary(zProb)
mod2<- larf(nettfa ~ inc + age + agesq + marr + fsize, treatment = p401k,
instrument = e401k, data = c401k, zProb = zProb)
summary(mod2)
mod3 <- larf(pira ~ inc + age + agesq + marr + fsize, p401k, instrument=e401k,
data = c401k)
summary(mod3)
mod4 <- larf(pira ~ inc + age + agesq + marr + fsize, p401k, e401k,
data = c401k, AME = TRUE)
summary(mod4)
?ivreg
mod5<-ivreg(nettfa ~p401k + inc + age + agesq + marr + fsize|e401k+ inc + age + agesq + marr + fsize ,
data = c401k)
summary(mod5)
rm(list=ls())
setwd("~/Desktop/IntroDS/Sesion10")
library(caret)
library(dplyr)
data_set <- read.table("bankruptcy_data.txt", sep="\t", header=TRUE) # datos de empresar que hacen bancarrota y algunas caracteristicas de cada empresar.
#queremos predecir si la empresa hace bancarrota o no:
table(data_set$class)
data_set$class <- factor(ifelse(data_set$class, "y", "n"), levels=c("y", "n"))
fitControl <- trainControl(method = "cv", #cv es cross-validation
number = 3, # cantidad de folds
verboseIter = TRUE) #para que imprima por qué vuelta va
# Ver ?train
# Pruebas de búsqueda de hiperparámetros (grid search). Cambia el hiperparametro cp.
rpart_fit <- train(x = data_set[,-ncol(data_set)], #variables predictoras
y = data_set$class, #variable a predecir
method = "rpart", #aca le indicamos el modelo. Puede ser rpart, knn, etc.
trControl = fitControl)
# Caret ya devuelve el mejor hiperparametro (el que da mejor accuracy) y entrena un modelo con ese hiperparametro
print(rpart_fit)  # Qué métricas calcula sobre validación? Accuracy y kappa.
rpart_fit$results[which.max(rpart_fit$results$Accuracy),]
rpart_fit <- train(x = data_set[,-ncol(data_set)], #variables predictoras
y = data_set$class, #variable a predecir
method = "rpart", #aca le indicamos el modelo. Puede ser rpart, knn, etc.
trControl = fitControl)
# Caret ya devuelve el mejor hiperparametro (el que da mejor accuracy) y entrena un modelo con ese hiperparametro
print(rpart_fit)  # Qué métricas calcula sobre validación? Accuracy y kappa.
rpart_fit$results[which.max(rpart_fit$results$Accuracy),]
# Probemos otra estructura de experimento: cambiamos el metodo de validacion
fitControl <- trainControl(method = "LGOCV",  # Dejamos un conjunto de validación fuera
number = 1,
p = 0.8, #porcentaje del dataset que va a training
classProbs = TRUE,  # Pedimos que devuelva las probabilidades estimadas
verboseIter = TRUE,
summaryFunction = twoClassSummary)
knn_fit <- train(x = data_set[,-ncol(data_set)],
y = data_set$class,
method = "knn",
tuneLength = 4,  # Pedimos que prueba 4 valores de vecinos
trControl = fitControl,
metric = "ROC")
print(knn_fit)
knn_fit$results[which.max(knn_fit$results$ROC),]
predict(rpart_fit, newdata = data_set)
predict(rpart_fit, newdata = data_set, type = "prob")
knn_fit_scaled <- train(x=data_set %>% select(-class), y=data_set$class,
method="knn",
trControl = fitControl,  # Mantenemos el trainControl de antes
preProcess = c("center","scale"), #Caret permite pre-procesar los datos. Aca le indicamos que centre y escale las X
metric = "ROC")
knn_fit_scaled$results[which.max(knn_fit_scaled$results$ROC),]
data_set_log <- data_set
for (v in setdiff(names(data_set_log), "class")) {
data_set_log[[v]] <- log(data_set_log[[v]] + 1 - min(data_set_log[[v]]))
}
knn_fit_log <- train(x=data_set_log %>% select(-class), y=data_set_log$class,
method="knn",
trControl = fitControl,
metric = "ROC")
nb_fit <- train(x = data_set %>% select(-class),
y = data_set$class,
method = "naive_bayes",
trControl = fitControl,
metric = "ROC")
when <- data.frame(time = factor(c("afternoon", "night", "afternoon",
"morning", "morning", "morning",
"morning", "afternoon", "afternoon")),
day = factor(c("Mon", "Mon", "Mon",
"Wed", "Wed", "Fri",
"Sat", "Sat", "Fri")),
num_var = c(1:9))
levels(when$day) <- list(Mon="Mon", Tue="Tue", Wed="Wed", Thu="Thu",
Fri="Fri", Sat="Sat", Sun="Sun")
head(when)
mainEffects <- dummyVars(~ day + time, data = when)
View(mainEffects)
mainEffects
predict(mainEffects, when)
View(when)
y <- factor(ifelse(sample(rep_len(c(0, 1), 2500)), "y", "n"))
X <- matrix(rnorm(25000000), ncol=10000)
View(X)
var_imp <- apply(X, 2, function(x) {t.test(x[y=="y"], x[y!="y"])$p.value})
top_vars <- order(var_imp)[c(1:150)]
var_imp
X <- data.frame(X[,top_vars])
names(X) <- paste("Col", c(1:ncol(X)), sep="")
names(X) <- paste("Col", c(1:ncol(X)), sep="")
X
fit_on <- list("holdout" = seq(1, nrow(X)/2))
val_on <- list("holdout" = seq(nrow(X)/2 + 1, nrow(X)))
fitControl <- trainControl(method = "cv",  ## El método da igual podría ser LOOCV
index = fit_on, indexOut = val_on,
verboseIter = TRUE)
xgb_sim_fit <- train(x = X,
y = y,
method="xgbTree", trControl=fitControl)
plot(density(var_imp))
