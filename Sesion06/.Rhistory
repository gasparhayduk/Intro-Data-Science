help()
seq(from = 100, to = 200, by = 5)
c(1:100)
n <- c(2, 3, 5)
b <- c(TRUE, FALSE, TRUE, FALSE, FALSE)
lista1 <- list(n, b, 3)
lista2 <- list(numVar = n, logVar = b, scalar = 3)
lista2[[2]]
lista2$logVar
lista2$logVar[3]
c(1,3)
c(1, 3, 5) == 3
mtcars
?mtcars
mtcars$mpg > 20
mtcars[mtcars$mpg > 20,]
mtcars$mpg > 20 & mtcars$mpg <= 25
mtcars[mtcars$mpg > 20 & mtcars$mpg <= 25,]
c(2,6)
c(1, 2, 5, 10) %in% c(5, 8, 10)
mtcars$carb %in% c(2,6)
?grepl
mtcars[grepl("Merc", rownames(mtcars))
mtcars[grepl("Merc", rownames(mtcars)),]
mtcars[grepl("Merc", rownames(mtcars)),]
?mtcars
?setwd
bicis <- read.table("recorrido-bicis-2015.csv", header = TRUE,
sep = ";", stringsAsFactors = TRUE)
setwd("C:/Desktop/intro_datascience")
getwd()
setwd("C:/Users/gasparhayduk/Desktop/intro_datascience")
getwd()
strptime("22:05:32", "%H:%M:%OS")
Sys.time("2022-03-10 22:05:32 -03")
class(strptime("22:05:32", "%H:%M:%OS"))
?POSIXlt
as.numeric(strptime("22:05:32", "%H:%M:%OS"))
setwd("C:/Users/gasparhayduk/Home/Desktop/intro_datascience")
setwd("C:Home/Desktop/intro_datascience")
setwd("C:Users/gasparhayduk/Home/Desktop/intro_datascience")
rm(list=ls())
rm(list=ls())
library(ggplot2); library(reshape2); library(rpart); library(kernlab)
installed.packages("kernlab")
install.packages("kernlab")
library(ggplot2); library(reshape2); library(rpart); library(kernlab)
setwd("~/Desktop/IntroDS/Sesion06")
data(spam)
View(spam)
?spam
set.seed(12345)
test_index <- sample(c(1:nrow(spam)), 1380) # %10 para testear.
train_data <- spam[-test_index,]
test_data  <- spam[test_index,]
train_acc <- c() # vector vacio donde guardaremos la accuracy para training
test_acc  <- c() # vector vacio donde guardaremos la accuracy para test
depths <- seq(1, 30, by=1)
for (d in depths) {
print(d) # en qué profundidad vamos
tree_fit <- rpart(type ~ ., data = train_data,
control = rpart.control(maxdepth = d, xval=0, minsplit = 1, minbucket=1, cp=0)) # entrenamos el arbol para la profundidad d
train_acc <- c(train_acc, mean(train_data$type == predict(tree_fit, train_data, type="class")))
test_acc  <- c(test_acc, mean(test_data$type == predict(tree_fit, test_data, type="class"))) #vemos la accuracy
}
experiment_data <- data.frame(depth = depths, train_acc, test_acc)
plot_data <- melt(experiment_data, id.vars="depth", value.name="Accuracy")
ggplot(data=plot_data, aes(x=depth, y=Accuracy, col=variable)) + geom_line()
data_set <- read.table("bankruptcy_data_red.txt", header=TRUE, sep="\t", stringsAsFactors = TRUE) # usamos el dataset de bankruptcy de la Sesion05.
data_set$class <- factor(ifelse(data_set$class, "bankruptcy", "no_bankruptcy"))
library(class)
data_set <- read.table("bankruptcy_data_red.txt", header=TRUE, sep="\t", stringsAsFactors = TRUE) # usamos el dataset de bankruptcy de la Sesion05.
data_set$class <- factor(ifelse(data_set$class, "bankruptcy", "no_bankruptcy"))
# Separo en training y holdout/validation:
holdout_index  <- sample(nrow(data_set), 200)  # Aproximadamente un 20% de las obs.
train_index  <- setdiff(1:nrow(data_set), holdout_index)
predictions <- knn(train_data[,-ncol(train_data)], val_data[,-ncol(val_data)], train_data$class, k=9)
val_data    <- data_set[holdout_index,] # validation data. Las posiciones que estan en holdout.
train_data  <- data_set[train_index,]
predictions <- knn(train_data[,-ncol(train_data)], val_data[,-ncol(val_data)], train_data$class, k=9)
print(mean(val_data$class == predictions))
library(dplyr)
vld_accuracy <- data.frame()
for (i in c(1:reps)) {
holdout_index  <- sample(nrow(data_set), 200)  # Aproximadamente un 20% de las obs.
train_index  <- setdiff(1:nrow(data_set), holdout_index)  # Aproximadamente un 20% de las obs.
val_data    <- data_set[holdout_index,]
train_data  <- data_set[train_index,]
predictions <- knn(train_data[,-ncol(train_data)], val_data[,-ncol(val_data)], train_data$class, k=9)
vld_accuracy  <- rbind(vld_accuracy,
data.frame(rep=i,  acc=mean(val_data$class == predictions)))
}
reps <- 30 # repetimos 30 veces lo de separar en training y validation.
vld_accuracy <- data.frame()
for (i in c(1:reps)) {
holdout_index  <- sample(nrow(data_set), 200)  # Aproximadamente un 20% de las obs.
train_index  <- setdiff(1:nrow(data_set), holdout_index)  # Aproximadamente un 20% de las obs.
val_data    <- data_set[holdout_index,]
train_data  <- data_set[train_index,]
predictions <- knn(train_data[,-ncol(train_data)], val_data[,-ncol(val_data)], train_data$class, k=9)
vld_accuracy  <- rbind(vld_accuracy,
data.frame(rep=i,  acc=mean(val_data$class == predictions)))
}
print(mean(vld_accuracy$acc))
loocv_acc <- rep(NA, nrow(train_data))
for (i in c(1:nrow(data_set))) {  # Itero sobre cada observación.
train_data <- data_set[-i,]
loocv_data  <- data_set[i,]
tmp_vd_pred <- knn(train_data[,-ncol(train_data)], loocv_data[,-ncol(val_data)], train_data$class, k=9)
loocv_acc[i] <- loocv_data$class == tmp_vd_pred
}
mean(loocv_acc)
