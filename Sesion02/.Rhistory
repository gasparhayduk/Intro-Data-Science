mean(dinero_2)
mean((dinero_1 - mean(dinero_1))^2)
var(dinero_1)
mean((dinero_2 - mean(dinero_2))^2)
var(dinero_1)
var(dinero_1)*(length(dinero_1)-1)/length(dinero_1)
h <- women$height
w <- women$weight
plot(h, w)
((h-mean(h)) %*% (w-mean(w))) / (length(w) - 1)
cov(h, w)
cov(data.frame(h, w))
cor(h, w)
cor(h, wKm)
wKm <- w / 2.2 # pasamos el peso de libras a kilogramo
cov(h, wKm)
cor(h, wKm)
cor(h, w)
cor(data.frame(h, wKm))
iris
cov(iris[iris$Species=="virginica", c(1:4)])
sum(diag(cov(iris[iris$Species=="virginica", c(1:4)])))
plot(iris[iris$Species=="virginica", c(1:4)])
head(USArrests)
?USArrests
head(USArrests)
cor(USArrests)
plot(USArrests)
?prcomp
pr.out <- prcomp(USArrests, center = TRUE, scale = TRUE)
class(pr.out)
str(pr.out)
pr.out$sdev
pr.out$rotation
USArrests_pc <- predict(pr.out, USArrests)
head(USArrests_pc)
biplot(pr.out, scale = 0)
prcomp(USArrests, center = TRUE, scale = TRUE)
prcomp(USArrests, center = TRUE, scale = FALSE)
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
plot(pve, xlab = "Principal Component",
ylab = "Proportion     Variance Explained",
ylim = c(0,1),type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = " Cumulative Proportion of Variance Explained",
ylim=c(0,1), type="b")
plot(pve, xlab = "Principal Component",
ylab = "Proportion     Variance Explained",
ylim = c(0,1),type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = " Cumulative Proportion of Variance Explained",
ylim=c(0,1), type="b")
head(USJudgeRatings)
?USJudgeRatings
head(USJudgeRatings)
cor(USJudgeRatings)
plot(USJudgeRatings)
pr.out <- prcomp(USJudgeRatings, center = TRUE, scale = TRUE)
pr.jd <- prcomp(USJudgeRatings, center = TRUE, scale = TRUE)
pr.jd$sdev
pr_jd <- prcomp(USJudgeRatings, center = TRUE, scale = TRUE)
pr_jd$sdev
pr_jd$rotation
USA_jd_pc <- predict(pr_jd,USJudgeRatings )
biplot(pr_jd, scale = 0)
pr_jd <- pr_jd$sdev^2
pve_jd <- pr_jd_var/sum(pr_jd_var)
pr_jd_var <- pr_jd$sdev^2
pr_jd <- prcomp(USJudgeRatings, center = TRUE, scale = TRUE)
pr_jd_var <- pr_jd$sdev^2
pve_jd <- pr_jd_var/sum(pr_jd_var)
plot(pve_jd, xlab = "Principal Component",
ylab = "Proportion     Variance Explained",
ylim = c(0,1),type = "b")
plot(cumsum(pve_jd), xlab = "Principal Component",
ylab = " Cumulative Proportion of Variance Explained",
ylim=c(0,1), type="b")
biplot(pr_jd, scale = 0)
biplot(pr_jd, scale = 0, cex=c(o.75,0.75))
biplot(pr_jd, scale = 0, cex=c(0.75,0.75))
install.packages('grf')
library(grf)
df <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv')
df <- df[complete.cases(df),]
split <- sample(c(FALSE, TRUE), nrow(df), replace = TRUE)
df.train <- df[split,]
df.hold <- df[!split,]
pctymle <- as.matrix(df.train$pctymle)
View(pctymle)
crmrte <- as.matrix(df.train$crmrte)
X <- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris +
avgsen + polpc + density + taxpc + factor(region) + factor(smsa) +
pctmin + wcon, data = df.train))
cf <- causal_forest(X,crmrte,pctymle)
View(cf)
effects <- predict(cf)$predictions
effects
cf
X.hold <- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris +
avgsen + polpc + density + taxpc + factor(region) + factor(smsa) +
pctmin + wcon, data = df.hold))
effects.hold <- predict(cf, X.hold)$predictions
effects.hold
X
View(pctymle)
cf
View(df)
str(df)
df$year
?`grf-package`
average_treatment_effect(cf), target.sample = "all")
average_treatment_effect(cf, target.sample = "all")
average_treatment_effect(cf, target.sample = df.hold)
average_treatment_effect(cf, target.sample = "df.hold")
average_treatment_effect(tau.forest, target.sample = "treated")
average_treatment_effect(cf, target.sample = "treated")
n <- 2000; p <- 10
rm(list=ls())
n <- 2000; p <- 10
X <- matrix(rnorm(n*p), n, p)
X
View(X)
X.test <- matrix(0, 101, p)
X.test[,1] <- seq(-2, 2, length.out = 101)
W <- rbinom(n, 1, 0.4 + 0.2 * (X[,1] > 0))
Y <- pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest <- causal_forest(X, Y, W)
tau.hat.oob <- predict(tau.forest)
hist(tau.hat.oob$predictions)
tau.hat <- predict(tau.forest, X.test)
plot(X.test[,1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2),
xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 2)
average_treatment_effect(tau.forest, target.sample = "all")
average_treatment_effect(tau.forest, target.sample = "treated")
tau.forest <- causal_forest(X, Y, W, num.trees = 4000)
tau.hat <- predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat <- sqrt(tau.hat$variance.estimates)
ylim <- range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2)
plot(X.test[,1], tau.hat$predictions, ylim = ylim, xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 1)
n <- 4000; p <- 20
X <- matrix(rnorm(n * p), n, p)
TAU <- 1 / (1 + exp(-X[, 3]))
W <- rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))
Y <- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)
forest.W <- regression_forest(X, W, tune.parameters = "all")
W.hat <- predict(forest.W)$predictions
forest.Y <- regression_forest(X, Y, tune.parameters = "all")
Y.hat <- predict(forest.Y)$predictions
forest.Y.varimp <- variable_importance(forest.Y)
forest.Y.varimp <- variable_importance(forest.Y)
selected.vars <- which(forest.Y.varimp / mean(forest.Y.varimp) > 0.2)
tau.forest <- causal_forest(X[, selected.vars], Y, W,
W.hat = W.hat, Y.hat = Y.hat,
tune.parameters = "all")
rm(list=ls())
library(grf)
df <- read.csv('https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv')
df <- df[complete.cases(df),]
split <- sample(c(FALSE, TRUE), nrow(df), replace = TRUE)
df.train <- df[split,]
df.hold <- df[!split,]
pctymle <- as.matrix(df.train$pctymle)
View(pctymle)
crmrte <- as.matrix(df.train$crmrte)
X <- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris +
avgsen + polpc + density + taxpc + factor(region) + factor(smsa) +
pctmin + wcon, data = df.train))
View(X)
cf <- causal_forest(X,crmrte,pctymle)
effects <- predict(cf)$predictions
X.hold <- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris +
avgsen + polpc + density + taxpc + factor(region) + factor(smsa) +
pctmin + wcon, data = df.hold))
effects.hold <- predict(cf, X.hold)$predictions
average_treatment_effect(cf, target.sample = "all")
SEs <- sqrt(predict(cf, X.hold, estimate.variance = TRUE)$variance.estimates)
rm(list=ls())
par(mfrow=c(1,1))
set.seed(1234)
datos_lineales <- data.frame(x1=runif(400, min=-1, max=1),
x2=runif(400, min=-1, max=1))
X <- as.matrix(datos_lineales[,c("x1", "x2")])
a1 <- 10 / 100
b1 <- -1.5
datos_lineales$y1 <- as.numeric((a1 + b1 * datos_lineales$x1) > datos_lineales$x2)
plot(datos_lineales[,c("x1", "x2")],
col=factor(datos_lineales$y1), pch=datos_lineales$y1 + 2)
sigmoid <- function(z) {
return(1/(1 + exp(-z)))
}
logistic_pred <- function(X, coefs, bias) {
return(sigmoid(X %*% coefs + bias))
}
logloss <- function(y, preds_probs) {
return(-(sum(log(preds_probs[y==1])) + sum(log(1-preds_probs[y==0]))) / length(y))
}
derivative_of_logistic <- function(X, y, preds_probs) {
derivative_coefs <- (t(X) %*% (preds_probs - y)) / length(y)
derivative_bias  <- mean(preds_probs - y)
return(list(derivative_coefs = derivative_coefs, derivative_bias=derivative_bias))
}
plot_logistic_frontier <- function(coefs, bias) {
a <- -bias/coefs[2]
b <- -coefs[1] / coefs[2]
abline(a, b, col="blue")
}
train_gd_logistic_with_plot <- function(X, y, coefs, bias, learning_rate = 0.1,
stopCriterion = 0.000001, verbose = TRUE,
print_every = 3000) {
end_while <- FALSE
preds_probs <- logistic_pred(X, coefs, bias)
i <- 1
while (!end_while) {
old_coefs <- coefs
old_bias <- bias
der_logistic <- derivative_of_logistic(X, y, preds_probs)
# Actualizo los coeficientes y el sesgo
coefs <- coefs - learning_rate * der_logistic$derivative_coefs
bias <- bias - learning_rate * der_logistic$derivative_bias
# Veo las nuevas predicciones
preds_probs <- logistic_pred(X, coefs, bias)
# Imprimo el nuevo error
if (verbose & !(i %% print_every)) {
print(logloss(y, preds_probs))
plot(X, col=factor(y), pch=y + 2, xlim=c(-1, 1), ylim=c(-1, 1))
plot_logistic_frontier(coefs, bias)
}
# Veo si parar el while
if (sqrt(sum((old_coefs - coefs) ^ 2) + (old_bias - bias) ^ 2) < stopCriterion) {
end_while <- TRUE
}
i <- i + 1
}
return(list(coefs=coefs, bias=bias))
}
coefs <- matrix(rnorm(ncol(datos_lineales[,c("x1", "x2")])), ncol = 1)
bias <- rnorm(1)
trained_logistic <- train_gd_logistic_with_plot(X,
datos_lineales$y1,
coefs, bias,
learning_rate = 0.01,
print_every=1000,
stopCriterion = 0.00005)
a2 <- 15 / 100
b2 <- -0.5
datos_lineales$y2 <- as.numeric((a2 + b2 * datos_lineales$x1) > datos_lineales$x2)
datos_lineales$y3 <- ifelse(!datos_lineales$y1 & datos_lineales$y2, 1, 0)
plot(datos_lineales[,c("x1", "x2")], col=factor(datos_lineales$y3),
pch=datos_lineales$y3 + 2)
one_hidden_layer_pred <- function(X, coefs1, bias1, coefs2, bias2) {
n_obs <- nrow(X)
# First layer
n_hidden <- nrow(coefs1)
z1 <- coefs1 %*% t(X) + matrix(rep(bias1, n_obs), nrow = n_hidden)
a1 <- sigmoid(z1)
# Second layer
n_out <- nrow(coefs2)
z2 <- coefs2 %*% a1 + matrix(rep(bias2, n_obs), nrow = n_out)
preds_probs <- t(sigmoid(z2))
return(list(preds_probs = preds_probs,
z1 = z1,
a1 = a1))
}
logloss_multiclass <- function(y, preds_probs) {
return(-sum(log(rowSums(preds_probs * y))) / nrow(y))
}
derivative_of_sigmoid <- function(z) {
return(sigmoid(z) * (1 - sigmoid(z)))
}
one_hidden_layer_pred <- function(X, coefs1, bias1, coefs2, bias2) {
n_obs <- nrow(X)
# First layer
n_hidden <- nrow(coefs1)
z1 <- coefs1 %*% t(X) + matrix(rep(bias1, n_obs), nrow = n_hidden)
a1 <- sigmoid(z1)
# Second layer
n_out <- nrow(coefs2)
z2 <- coefs2 %*% a1 + matrix(rep(bias2, n_obs), nrow = n_out)
preds_probs <- t(sigmoid(z2))
return(list(preds_probs = preds_probs,
z1 = z1,
a1 = a1))
}
logloss_multiclass <- function(y, preds_probs) {
return(-sum(log(rowSums(preds_probs * y))) / nrow(y))
}
derivative_of_sigmoid <- function(z) {
return(sigmoid(z) * (1 - sigmoid(z)))
}
derivative_of_one_hidden_layer <- function(X, y, preds_probs, coefs2, a1, z1) {
n_obs <- nrow(X)
n_out <- ncol(preds_probs)
# Derivative of output layer
dz2 <- t(preds_probs) - t(y)
derivative_coefs2 <- dz2 %*% t(a1) / n_obs
derivative_bias2  <- matrix(rowMeans(t(preds_probs) - t(y)), ncol=1)
# Derivative of hidden layer
dz1 <- t(coefs2) %*% dz2 * derivative_of_sigmoid(z1)
derivative_coefs1 <- dz1 %*% X / n_obs
derivative_bias1 <- matrix(rowMeans(dz1), ncol = 1)
return(list(derivative_coefs1 = derivative_coefs1,
derivative_bias1 = derivative_bias1,
derivative_coefs2 = derivative_coefs2,
derivative_bias2 = derivative_bias2))
}
train_gd_one_hidden <- function(X, y, coefs1, bias1, coefs2, bias2,
learning_rate = 0.1,
stopCriterion = 0.000001, verbose = TRUE,
print_every = 3000,
twoPlots = FALSE,
multiclass = FALSE,
savePlot = FALSE,
noPlot = FALSE,
maxEpochs = NULL) {
if (twoPlots) {par(mfrow=c(1,2))} else {par(mfrow=c(1,1))}
end_while <- FALSE
preds_and_cache <- one_hidden_layer_pred(X, coefs1, bias1, coefs2, bias2)
i <- 1
j <- 1
while (!end_while) {
old_coefs1 <- coefs1
old_bias1 <- bias1
old_coefs2 <- coefs2
old_bias2 <- bias2
der_one_hidden <- derivative_of_one_hidden_layer(X,
y,
preds_and_cache$preds_probs,
coefs2,
preds_and_cache$a1,
preds_and_cache$z1)
# Actualizo los coeficientes y el sesgo
coefs1 <- coefs1 - learning_rate * der_one_hidden$derivative_coefs1
bias1 <- bias1 - learning_rate * der_one_hidden$derivative_bias1
coefs2 <- coefs2 - learning_rate * der_one_hidden$derivative_coefs2
bias2 <- bias2 - learning_rate * der_one_hidden$derivative_bias2
# Veo las nuevas predicciones
preds_and_cache <- one_hidden_layer_pred(X, coefs1, bias1, coefs2, bias2)
# Imprimo el nuevo error
if (verbose & !(i %% print_every)) {
if (!multiclass) {
ll <- logloss(y, preds_and_cache$preds_probs)
} else {
ll <- logloss_multiclass(y, preds_and_cache$preds_probs)
}
print(ll)
if (!noPlot) {
if (savePlot) {
jpeg(paste('nnet_gif/one_hidden_', sprintf("%06d", j), '.jpg', sep=""),
width = 480*2, height = 480)
j <- j + 1
}
if (twoPlots) {
par(mfrow=c(1,2))} else {par(mfrow=c(1,1))
}
if (!multiclass) {
plot(X, col=factor(y), xlim=c(-1, 1), ylim=c(-1, 1), pch=y + 2)
} else {
y_plot <- apply(y, 1, function(x) which(x==1))-1
plot(X, col=factor(y_plot), xlim=c(-1, 1), ylim=c(-1, 1), pch=y_plot + 2)
}
# Plot of features
for (k in c(1:nrow(coefs1))) {
plot_logistic_frontier(coefs1[k,], bias1[k])
}
# Plot if hidden activation space
if (twoPlots) {
plot(t(preds_and_cache$a1), col=factor(y), pch=y + 2, xlim=c(0, 1),
ylim=c(0, 1), xlab="a1", ylab="a2")
for (k in c(1:nrow(coefs2))) {
plot_logistic_frontier(coefs2[k,], bias2[k])
}
}
if (savePlot) {
dev.off()
if (ll < 0.005) {return()}
}
}
}
# Veo si parar el while
if (sqrt(sum((old_coefs1 - coefs1) ^ 2) + sum((old_bias1 - bias1) ^ 2) +
sum((old_coefs2 - coefs2) ^ 2) + sum((old_bias2 - bias2) ^ 2)) < stopCriterion) {
end_while <- TRUE
} else if (!is.null(maxEpochs)) {
if (i == maxEpochs) {
end_while <- TRUE
}
}
i <- i + 1
}
par(mfrow=c(1,1))
return(list(coefs1=coefs1, bias1=bias1,
coefs2=coefs2, bias2=bias2))
}
y <- matrix(datos_lineales[,"y3"], ncol=1)
n_hidden <- 2
n_vars <- 2
n_out <- 1
coefs1 <- matrix(3 * rnorm(n_vars * n_hidden), nrow = n_hidden) * 0.3
bias1 <- matrix(3 * rnorm(n_hidden), ncol = 1) * 0.1
coefs2 <- matrix(3 * rnorm(n_hidden * n_out), nrow = n_out) * 0.3
bias2 <- matrix(3 * rnorm(n_out), ncol = 1) * 0.1
train_gd_one_hidden(X, y, coefs1, bias1, coefs2, bias2, learning_rate = 0.6,
stopCriterion = 0.00001, verbose = TRUE, print_every = 200, twoPlots=TRUE)
break
datos_lineales$y4 <- as.numeric(apply(datos_lineales[,c("x1", "x2")], 1,
function(x) sqrt(sum(x^2))) < 0.55)
plot(datos_lineales[,c("x1", "x2")], col=factor(datos_lineales$y4),
pch=datos_lineales$y4 + 2)
y <- matrix(datos_lineales[,"y4"], ncol=1)
n_hidden <- 5
n_vars <- 2
n_out <- 1
coefs1 <- matrix(3 * rnorm(n_vars * n_hidden), nrow = n_hidden) * 0.3
bias1 <- matrix(3 * rnorm(n_hidden), ncol = 1) * 0.1
coefs2 <- matrix(rnorm(n_hidden * n_out), nrow = n_out) * 0.3
bias2 <- matrix(rnorm(n_out), ncol = 1) * 0.1
train_gd_one_hidden(X, y, coefs1, bias1, coefs2, bias2, learning_rate = 0.4,
stopCriterion = 0.000001, verbose = TRUE, print_every = 300)
a3 <- 120 / 100
b3 <- 1.5
datos_lineales$y5 <- as.numeric((a3 + b3 * datos_lineales$x1) > datos_lineales$x2)
datos_lineales$y6 <- ifelse(!datos_lineales$y5, 0,
ifelse(!datos_lineales$y1 & datos_lineales$y2, 1, 2))
plot(datos_lineales[,c("x1", "x2")], col=factor(datos_lineales$y6),
pch=datos_lineales$y6 + 2)
n_hidden <- 3
n_vars <- 2
n_out <- 3
coefs1 <- matrix(3 * rnorm(n_vars * n_hidden), nrow = n_hidden) * 0.5
bias1 <- matrix(3 * rnorm(n_hidden), ncol = 1) * 0.1
coefs2 <- matrix(3 * rnorm(n_hidden * n_out), nrow = n_out) * 0.5
bias2 <- matrix(3 * rnorm(n_out), ncol = 1) * 0.1
y <- matrix(t(sapply(datos_lineales$y6 + 1, function(x) as.numeric(seq(1, 3)==x))), ncol=n_out)
train_gd_one_hidden(X, y, coefs1, bias1, coefs2, bias2, learning_rate = 0.5,
stopCriterion = 0.000001, verbose = TRUE, print_every = 300, multiclass=TRUE)
plot(iris[,c(1:4)], pch=as.numeric(iris$Species), col=iris$Species)
valid_index <- sample(c(1:nrow(iris)), 50)
iris_shuffled <- iris[sample(c(1:nrow(iris))),]
scaled_iris_X <- scale(iris_shuffled[,-5], center=TRUE, scale=TRUE)
train_X <- as.matrix(scaled_iris_X[-valid_index,])
valid_X <- as.matrix(scaled_iris_X[valid_index,])
iris_y <- matrix(t(sapply(as.numeric(iris_shuffled$Species),
function(x) as.numeric(seq(1, 3)==x))), ncol=n_out)
train_y <- iris_y[-valid_index,]
valid_y <- iris_y[valid_index,]
n_hidden <- 10
n_vars <- 4
n_out <- 3
coefs1 <- matrix(rnorm(n_vars * n_hidden), nrow = n_hidden) * 0.5
bias1 <- matrix(rnorm(n_hidden), ncol = 1) * 0.1
coefs2 <- matrix(rnorm(n_hidden * n_out), nrow = n_out) * 0.5
bias2 <- matrix(rnorm(n_out), ncol = 1) * 0.1
iris_trained <- train_gd_one_hidden(train_X, train_y, coefs1, bias1, coefs2, bias2,
learning_rate = 0.4, stopCriterion = 0.00001,
verbose = TRUE, print_every = 300, multiclass=TRUE,
noPlot=TRUE, maxEpochs = 80000)
y_pred <- one_hidden_layer_pred(valid_X, iris_trained$coefs1, iris_trained$bias1,
iris_trained$coefs2, iris_trained$bias2)$preds_probs
y_pred
mean(apply(y_pred, 1, which.max) == apply(valid_y, 1, function(x) which(x==1)))
(5*7)*3*3
mean(c(1,2,3,4,5))
mean(c(1,3,5,7,9,94836363))
var(c(1,3,5,7,9,94836363))
sd(c(1,3,5,7,9,94836363))
setwd("~/Desktop/IntroDS/Sesion02")
bicis <- read.table("recorrido-bicis-2015.csv", header = TRUE,
sep = ";", stringsAsFactors = TRUE)
ggal <- read.table("https://query1.finance.yahoo.com/v7/finance/download/GGAL?period1=1616358367&period2=1647894367&interval=1d&events=history",
header=TRUE, sep=",")
#notar que buscamos los datos en una direccion de internet. Es la direccion de yahoo finance.
bbva <- read.table("https://query1.finance.yahoo.com/v7/finance/download/BBAR?period1=1616358367&period2=1647894367&interval=1d",
header=TRUE, sep=",")
ggal$quote <- factor("GGAL")
bbva$quote <- factor("BBVA")
head(airquality)
aql <- melt(airquality)
library("reshape2")
head(airquality)
head(airquality)
aql <- melt(airquality, id.vars = c("Month", "Day"),
measure.vars = c("Wind", "Temp"), #solo le pasamos estas variables
variable.name = "climate_variable",
value.name = "climate_value")
View(aql)
aql <- melt(airquality, id.vars = c("Month", "Day"),
variable.name = "climate_variable", #identificador para la variable
value.name = "climate_value")
aqw <- dcast(aql, Month + Day ~ climate_variable,
value.var = "climate_value")
View(aql)
rm(list=ls())
library(MASS)
head(Boston)
plot(Boston[,c(1:5)])
clusters <- kmeans(Boston, centers=7, iter.max=30,  nstart=20)
clusters$centers
clusters$cluster
table(clusters$cluster)
evol_variabilidad <- data.frame()
for (k in c(1:20)) {
clusters <-kmeans(Boston, centers=k, iter.max=30,  nstart=20)
evol_variabilidad <- rbind(evol_variabilidad,
data.frame(k=k,
var=clusters$tot.withinss)) # lo ultimo es la funcion a optimizar.
}
plot(c(1:20), evol_variabilidad$var, type="o", xlab="# Clusters", ylab="tot.withinss")
