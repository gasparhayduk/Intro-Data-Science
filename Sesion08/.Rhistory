help()
seq(from = 100, to = 200, by = 5)
c(1:100)
n <- c(2, 3, 5)
b <- c(TRUE, FALSE, TRUE, FALSE, FALSE)
lista1 <- list(n, b, 3)
lista2 <- list(numVar = n, logVar = b, scalar = 3)
lista2[[2]]
lista2$logVar
lista2$logVar[3]
c(1,3)
c(1, 3, 5) == 3
mtcars
?mtcars
mtcars$mpg > 20
mtcars[mtcars$mpg > 20,]
mtcars$mpg > 20 & mtcars$mpg <= 25
mtcars[mtcars$mpg > 20 & mtcars$mpg <= 25,]
c(2,6)
c(1, 2, 5, 10) %in% c(5, 8, 10)
mtcars$carb %in% c(2,6)
?grepl
mtcars[grepl("Merc", rownames(mtcars))
mtcars[grepl("Merc", rownames(mtcars)),]
mtcars[grepl("Merc", rownames(mtcars)),]
?mtcars
?setwd
bicis <- read.table("recorrido-bicis-2015.csv", header = TRUE,
sep = ";", stringsAsFactors = TRUE)
setwd("C:/Desktop/intro_datascience")
getwd()
setwd("C:/Users/gasparhayduk/Desktop/intro_datascience")
getwd()
strptime("22:05:32", "%H:%M:%OS")
Sys.time("2022-03-10 22:05:32 -03")
class(strptime("22:05:32", "%H:%M:%OS"))
?POSIXlt
as.numeric(strptime("22:05:32", "%H:%M:%OS"))
setwd("C:/Users/gasparhayduk/Home/Desktop/intro_datascience")
setwd("C:Home/Desktop/intro_datascience")
setwd("C:Users/gasparhayduk/Home/Desktop/intro_datascience")
rm(list=ls())
rm(list=ls())
install.packages("xgboost")
library("pROC")
install.packages("caret")
library("caret")
library("ggplot2")
library("dplyr")
setwd("~/Desktop/IntroDS/Sesion08")
rm(list=ls())
setwd("~/Desktop/IntroDS/Sesion08")
data_set <- read.table("weatherAUS.csv", sep=",", header=TRUE)
View(data_set)
str(data_set)
prop.table(table(data_set$RainTomorrow))
prop.table(table(RainTomorrow=data_set$RainTomorrow, RainToday=data_set$RainToday), 2)
data_set$Date <- as.Date(data_set$Date)
data_set$year <- as.numeric(format((data_set$Date), "%Y"))
data_set$month <- as.numeric(format((data_set$Date), "%m"))
data_set$Date <- as.numeric(data_set$Date)  # Para capturar alguna tendencia
ggplot(data_set, aes(x=RainTomorrow, y=MaxTemp)) + geom_boxplot()
ggplot(data_set %>% group_by(month, RainTomorrow) %>%
summarize(MaxTemp=mean(MaxTemp, na.rm = TRUE)),
aes(x=month, y=MaxTemp, color=RainTomorrow)) + geom_line()
ggplot(data_set, aes(x=RainTomorrow, y=WindSpeed9am)) +
geom_boxplot() + facet_wrap(~WindDir9am, scale="free_y")
ggplot(data_set %>% group_by(Location) %>%
summarize(RainTomorrow = mean(RainTomorrow=="Yes")) %>%
mutate(Location=reorder(Location, RainTomorrow)),
aes(x=Location, y=RainTomorrow)) +
geom_bar(position=position_dodge(), stat="identity") + coord_flip()
data_set$RainTomorrow <- ifelse(data_set$RainTomorrow=="Yes", 1, 0)  # Predice 0 ó 1.
data_set$RISK_MM <- NULL  # La página dice que es una variable a quitar si no se quiere hacer trampa
table(data_set$year)
train_set <-  data_set %>% filter(year < 2016)  # Dejo 2017 para testear
val_set <- data_set %>% filter(year == 2016)  # Dejo 2016 para validar
test_set <- data_set %>% filter(year == 2017)  # Dejo 2017 para testear
one_hot_model <- dummyVars(~ ., data = train_set)
train_set <- predict(one_hot_model, train_set)  # Ojo, ya no son data.frames, son matrices
val_set <- predict(one_hot_model, val_set)
test_set <- predict(one_hot_model, test_set)
head(train_set)
dtrain <- xgb.DMatrix(data = train_set[,setdiff(colnames(train_set), "RainTomorrow")],
label = train_set[,"RainTomorrow"])
dvalid <- xgb.DMatrix(data = val_set[,setdiff(colnames(val_set), "RainTomorrow")],
label = val_set[,"RainTomorrow"]) #labes es la Y
library("xgboost")
dtrain <- xgb.DMatrix(data = train_set[,setdiff(colnames(train_set), "RainTomorrow")],
label = train_set[,"RainTomorrow"])
dvalid <- xgb.DMatrix(data = val_set[,setdiff(colnames(val_set), "RainTomorrow")],
label = val_set[,"RainTomorrow"])
model_1 <- xgb.train(data = dtrain,
params = list(max_depth = 5,  # Profundidad máxima
eta = 0.1,  # Learnig rate
gamma = 0.2,  # Reducción mínima requerida en el erro para aceptar la partición
colsample_bytree = 0.9,  # Columnas que se muestrean al construir casa árbol
subsample = 0.9,  # Observaciones que se muestrean al construir cada árbol
min_child_weight = 1),  # Equivalente a cantidad de observaciones que debe tener una hoja para ser aceptada
nrounds = 100,  # Cantidad de árboles a entrenar
watchlist = list(train = dtrain, valid = dvalid),
objective = "binary:logistic", #predecir clasificacion binaria. ver en help qué poner en objective para regresion.
eval.metric = "auc",
print_every_n = 10)
